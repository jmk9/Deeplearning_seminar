{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True) #편향가중치 추가\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "       \n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
    "    \n",
    "        #print(W)\n",
    "        #print(self.weight)\n",
    "        #print(self.bias)\n",
    "        #print(self.parameters.append(self.weight))\n",
    "        #print(self.parameters.append(self.bias))\n",
    "        #print(input.mm(self.weight)+self.bias.expand(0,len(input.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # 이 초기화 스타일은 word2vec에서 가져온 겁니다.\n",
    "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화기 - 배치 크기가 1인 경사하강법\n",
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "        \n",
    "    def step(self, zero=True):\n",
    "        \n",
    "        for p in self.parameters:\n",
    "            \n",
    "            p.data -= p.grad.data * self.alpha #갱신단계\n",
    "            \n",
    "            if(zero):\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "===================\n",
      "[2.33428272]\n",
      "[0.06743796]\n",
      "[0.0521849]\n",
      "[0.04079507]\n",
      "[0.03184365]\n",
      "[0.02479336]\n",
      "[0.01925443]\n",
      "[0.01491699]\n",
      "[0.01153118]\n",
      "[0.00889602]\n"
     ]
    }
   ],
   "source": [
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "    \n",
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "print(data)\n",
    "print(target)\n",
    "print('===================')\n",
    "\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # Compare\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 16: The Cross Entropy Layer"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy \n",
    "\n",
    "\n",
    "예측 모형은 실제 분포인 q 를 모르고, 모델링을 하여 q 분포를 예측하고자 하는 것이다. 예측 모델링을 통해 구한 분포를 p(x) 라고 해보자. 실제 분포인 q를 예측하는 p 분포를 만들었을 때, 이 때 cross-entropy 는 아래와 같이 정의된다. \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "이 때, q와 p가 모두 식에 들어가기 때문에, cross-entropy 라는 이름이 붙었다고 할 수 있다. 머신러닝을 통한 예측 모형에서 훈련 데이터에서는 실제 분포인 p 를 알 수 있기 때문에 cross-entropy 를 계산할 수 있다.  \n",
    "**즉, 훈련 데이터를 사용한 예측 모형에서 cross-entropy 는 실제 값과 예측값의 차이 (dissimilarity) 를 계산하는데 사용할 수 있다는 것이다.** 또한, Cross-entropy > entropy 이다. \n",
    "\n",
    "\n",
    "\n",
    "예를 들어, 가방에 0.8/0.1/0.1 의 비율로, 빨간/녹색/노랑 공이 들어가 있다고 하자, 하지만 직감에는 0.2/0.2/0.6의 비율로 들어가 있을 것 같다. 이 때, entropy 와 cross-entropy 는 아래와 같이 계산된다. \n",
    "\n",
    "\n",
    "\n",
    "H(q)=−[0.8log(0.8)+0.1log(0.1)+0.1log(0.1)]=0.63\n",
    "Hp(q)=−[0.8log(0.2)+0.1log(0.2)+0.1log(0.6)]=1.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동미분업그레이드\n",
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "        \n",
    "       # =============================================================== #\n",
    "                #Tensor 클래스의 backward() 메소드 중 일부\n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "     # =============================================================== #\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    " # =============================================================== #\n",
    "    # 역자주_다음 코드는 Tensor 클래스 멤버인 cross_entropy()이다.\n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "        \n",
    "        # print(t)\n",
    "        print(p)\n",
    "        # print(target_dist)\n",
    "        # print(loss) \n",
    "        \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "\n",
    "        return Tensor(loss),t,p,target_dist,loss\n",
    "\n",
    "    \n",
    " # =============================================================== #      \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss 클래스 중 일부\n",
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 2]\n",
      "[0 1 0 1]\n",
      "\n",
      "<__main__.Sequential object at 0x000002228E604248>\n",
      "<__main__.CrossEntropyLoss object at 0x000002228E0C7248>\n",
      "\n",
      "1.3885032434928422\n",
      "0.9558181509266037\n",
      "0.6823083585795604\n",
      "0.5095259967493119\n",
      "0.39574491472895856\n",
      "0.31752527285348264\n",
      "0.2617222861964216\n",
      "0.22061283923954234\n",
      "0.18946427334830068\n",
      "0.16527389263866668\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "\n",
    "# data indices\n",
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "print(data)\n",
    "\n",
    "# target indices\n",
    "target = Tensor(np.array([0,1,0,1]), autograd=True)\n",
    "print(target)\n",
    "print()\n",
    "\n",
    "model = Sequential([Embedding(3,3), Tanh(), Linear(3,4)])\n",
    "criterion = CrossEntropyLoss()\n",
    "print(t)\n",
    "print(p)\n",
    "print(target_dist)\n",
    "print()\n",
    "print(model)\n",
    "print(criterion)\n",
    "print()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # Compare\n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 과거 여러 신경망에 탑재했던 교차 엔트로피(cross entropy) 논리를 그대로 활용한 **새로운 손실 함수**가 생겼다.  \n",
    "* 다른 손실 함수와 다른 점은 <U>최종 softmax와 손실 계산 모두, 손실 클래스 안에 위치</U>한다는 것=>심층 신경망에서는 평범한 관례  \n",
    "* 신경망을 종료하고 교체 엔트로피로 학습시키고자 할 때는 순전파 단계에서 softmax를 제외하고,\n",
    "* 손실함수의 일부로써 softmax를 자동으로 실행하는 교차 트로피 클래스를 호출할 수 있다 .\n",
    "\n",
    "* 순전파와 역전파를 2개의 다른 모듈로 분리할 때보다 **교차-엔트로피함수 내부**에서 softmax의 경사도와 음의 로그 기능도 모두를 훨씬 빨리 계산해낸다.==> '경사도를 빠르게 구하는 계산법(shortcut for math)'와 관련이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 17: The Recurrent Neural Network Layer\n",
    "#### 여러 계층을 결합하면 시계열을  학습할 수 있다.\n",
    "* 조그만 유형의 계층 여럿을 결합하는 계층을 하나 더 만들어보자.  \n",
    "* 바로, **순환계층(recurrent layer)**\n",
    "=> 순환 계층은 3개의 선형 계층을 이용해서 구축하며 .forward()메소드는 이전 은닉 상태에서 나온 출력과 현재 학습 데이터에서 나온 입력 모두를 취한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-82fb3b42cd76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mRNNCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-82fb3b42cd76>\u001b[0m in \u001b[0;36mRNNCell\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_hidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_prev_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_hh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_ih\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrom_prev_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "class RNNCell(Layer):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()        \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    #print(from_prev_hidden = self.w_hh.forward(hidden))\n",
    "    #print(combined = self.w_ih.forward(input) + from_prev_hidden)\n",
    "    #print(new_hidden = self.activation.forward(combined))\n",
    "    #print(output = self.w_ho.forward(new_hidden))\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 시간 단계에서 시간 단계로 전해시는 상태 벡터를 갖고 있다. 위 예제에서는 **hidden**이라는 변수가 바로 **상태 벡터**이다.  \n",
    "또한, hidden은 forward()함수에 대한 입력 매개변수이자, 출력변수이기도 하다.  \n",
    "\n",
    "* RNN 역시 여러 가지 가중치 행렬을 가진다.\n",
    "* 어떤 가중치 행렬은 입력 벡터를 은닉 벡터에 매핑(입력 데이터 처리)하고, \n",
    "* 또 어떤 가중치 행렬은 은닉 벡터에서 은닉 벡터로 매핑(이전 은닉 벡터에 기반해서 각 은닉 벡터를 갱신)\n",
    "* 은닉-출력 계층은 은닉 벡터에 기반해서 예측하는 법을 학습한다.\n",
    "\n",
    "**RNNCell구현은 이 세 가지를 모두 포함한다.(self.w_ih 계층은 입력-은닉 계층, self.w_hh는 은닉-은닉 계층, self.w_ho는 은닉-출력 계층)**\n",
    "\n",
    "* self.w_ih의 입력 키기와 self.w_ho의 출력 크기는 모두 어휘의 크기와 같다. \n",
    "* 다른 나머지 차원수는 n_hidden 매개변수를 이용해서 조정할 수 있다.\n",
    "\n",
    "* activation 입력 매개변수는 어떤 비선형성이 어떤 시간 단계에서 은닉 벡터에 적용되었는지를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "\n",
    "new_tokens = list()\n",
    "for line in tokens:\n",
    "    new_tokens.append(['-'] * (6 - len(line)) + line)\n",
    "\n",
    "tokens = new_tokens\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "indices = list()\n",
    "for line in tokens:\n",
    "    idx = list()\n",
    "    for w in line:\n",
    "        idx.append(word2index[w])\n",
    "    indices.append(idx)\n",
    "\n",
    "data = np.array(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 앞 장에서 수행했던 과업에 맞게 신경망을 학습시킬 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Embedding object at 0x000002228E0DD5C8>\n",
      "<__main__.RNNCell object at 0x000002228E0DD908>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed = Embedding(vocab_size=len(vocab),dim=16)\n",
    "model = RNNCell(n_inputs=16, n_hidden=16, n_output=len(vocab))\n",
    "print(embed)\n",
    "print(model)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의 개수의 셀을 함께 설정할 수 있는 능력을 제공하는 어떤 계층을 만들어냈다면, 그 계층은 RNN이라는 이름을 얻게 되고 n_layers는 입력 매개변수가 되었을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4509692171421455 % Correct: 0.0\n",
      "Loss: 0.17390647939421655 % Correct: 0.27\n",
      "Loss: 0.1551120990569272 % Correct: 0.33\n",
      "Loss: 0.13946057897029826 % Correct: 0.37\n",
      "Loss: 0.13760040026494857 % Correct: 0.37\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "    batch_size = 100\n",
    "    total_loss = 0\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "\n",
    "    for t in range(5):\n",
    "        input = Tensor(data[0:batch_size,t], autograd=True)\n",
    "        rnn_input = embed.forward(input=input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "    target = Tensor(data[0:batch_size,t+1], autograd=True)    \n",
    "    loss = criterion.forward(output, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.data\n",
    "    if(iter % 200 == 0):\n",
    "        p_correct = (target.data == np.argmax(output.data,axis=1)).mean()\n",
    "        print(\"Loss:\",total_loss / (len(data)/batch_size),\"% Correct:\",p_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\n",
      "4.723707807286633\n",
      "\n",
      "Context: - mary moved to the \n",
      "True: bathroom.\n",
      "Pred: \tkitchen\t14\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "hidden = model.init_hidden(batch_size=batch_size)\n",
    "for t in range(5):\n",
    "    input = Tensor(data[0:batch_size,t], autograd=True)\n",
    "    rnn_input = embed.forward(input=input)\n",
    "    output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "target = Tensor(data[0:batch_size,t+1], autograd=True)    \n",
    "loss = criterion.forward(output, target)\n",
    "print(target)\n",
    "print(loss)\n",
    "print()\n",
    "\n",
    "ctx = \"\"\n",
    "for idx in data[0:batch_size][0][0:-1]:\n",
    "    ctx += vocab[idx] + \" \"\n",
    "print(\"Context:\",ctx)\n",
    "print(\"True:\",vocab[target.data[0]])\n",
    "print(\"Pred:\", vocab[output.data.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(37%의 정확도, 장난감 과업에 대해선 거의 완벽한 수준)=>mary가 갈 장소를 그럴듯하게 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요약\n",
    "#### 프레임워크는 전파와 전파 논리의 효율적이며 간편한 추상화이다.\n",
    "* 프레임워크는 코드에 관해 읽기는 수월하게, 작성은 빠르게, 실행도 빠르게 (내장 최적화 기능을 통해),버그는 적게 만들어준다.\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
