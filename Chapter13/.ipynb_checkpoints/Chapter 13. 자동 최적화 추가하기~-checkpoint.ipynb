{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. 자동 최적화(Optimizer) 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <앞서 했던것들 - Class Tensor 선언>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## '확률적 경사하강법 최적화기 (SGD Optimizer)'를 만들어 봅시다!\n",
    "\n",
    "\n",
    "### SGD(Stochastic Gradient Descent, 확률적 경사하강법) 란?\n",
    "\n",
    " 데이터 세트에서 무작위로 선택한 데이터 하나를 가지고 학습을 합니다. 노이즈는 많이 생기겠지만 지속적으로 무작위 데이터를 선택할 경우, 훨씬 적은 데이터 세트로 중요한 평균값을 추정할 수 있습니다.\n",
    "(그냥 믿고 가야합니다. 모집단에서 샘플링하는 행위를 무수히 많이 했을 때 모집단의 평균에 수렴하는 원리)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Class SGD 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():  #object 상속 - 호환성을 위함, 안써도 python 3 에서는 큰 문제 없음\n",
    "    def __init__(self, parameters, alpha = 0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def zero(self):  #미분데이터를 0으로 만듬\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *=0\n",
    "            \n",
    "    def step(self, zero = True):\n",
    "        for p in self.parameters:\n",
    "            p.data -=p.grad.data*self.alpha\n",
    "            \n",
    "        if(zero):\n",
    "            p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literation :  1\n",
      "pred :  [[0.        ]\n",
      " [0.40915397]\n",
      " [0.46517944]\n",
      " [0.8743334 ]]\n",
      "loss :  [0.58128304]\n",
      "====================\n",
      "literation :  2\n",
      "pred :  [[0.        ]\n",
      " [0.50658339]\n",
      " [0.49639968]\n",
      " [1.00298307]]\n",
      "loss :  [0.48988149]\n",
      "====================\n",
      "literation :  3\n",
      "pred :  [[0.        ]\n",
      " [0.58279912]\n",
      " [0.42146354]\n",
      " [1.00426266]]\n",
      "loss :  [0.35170626]\n",
      "====================\n",
      "literation :  4\n",
      "pred :  [[0.        ]\n",
      " [0.71187637]\n",
      " [0.32818368]\n",
      " [1.04006005]]\n",
      "loss :  [0.19232457]\n",
      "====================\n",
      "literation :  5\n",
      "pred :  [[0.        ]\n",
      " [0.85957508]\n",
      " [0.19362989]\n",
      " [1.05320497]]\n",
      "loss :  [0.06004246]\n",
      "====================\n",
      "literation :  6\n",
      "pred :  [[0.        ]\n",
      " [1.00555017]\n",
      " [0.03172923]\n",
      " [1.0372794 ]]\n",
      "loss :  [0.0024273]\n",
      "====================\n",
      "literation :  7\n",
      "pred :  [[ 0.        ]\n",
      " [ 1.11588583]\n",
      " [-0.12827964]\n",
      " [ 0.98760619]]\n",
      "loss :  [0.0300388]\n",
      "====================\n",
      "literation :  8\n",
      "pred :  [[ 0.        ]\n",
      " [ 1.16993727]\n",
      " [-0.24576217]\n",
      " [ 0.9241751 ]]\n",
      "loss :  [0.09502714]\n",
      "====================\n",
      "literation :  9\n",
      "pred :  [[ 0.        ]\n",
      " [ 1.17146731]\n",
      " [-0.29749736]\n",
      " [ 0.87396995]]\n",
      "loss :  [0.13378929]\n",
      "====================\n",
      "literation :  10\n",
      "pred :  [[ 0.        ]\n",
      " [ 1.14682355]\n",
      " [-0.29245105]\n",
      " [ 0.85437251]]\n",
      "loss :  [0.12829214]\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0) #random의 seed 설정 - random하게 나오는 수가 항상 같도록 함\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "#weight를 random하게 설정\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "optim = SGD(parameters=w, alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print('literation : ', i+1)\n",
    "    \n",
    "    # 예측\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    print('pred : ', pred)\n",
    "    \n",
    "    # loss Function\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    print('loss : ',loss)\n",
    "    \n",
    "    # 학습\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    \n",
    "    print('====================')\n",
    "\n",
    "    \n",
    "data = Tensor(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. 계층 형식 지원하기\n",
    "\n",
    "## 상용 프레임워크가 지원하는 계층 형식을 우리 프레임 워크에 추가해 봅시다\n",
    "### 계층 추상화(Layer Abstraction)란?\n",
    " 거의 모든 프레임 워크에서 가장 일반적인 추상화는 계층 추상화. 계층 추상화란 자주 사용되는 순전파 기법들을 보다 간단하게 API안에 모아놓은 API 집합입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))  #왜 곱해줬을까\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True) #편향 가중치 추가\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight)+self.bias.expand(0,len(input.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear 클레스 내부에 새롭게 추가된 개념은 없습니다. \n",
    "가중치가 클래스 내부의 변수로 들어왔고(편향 가중치를 추가), 가중치와 편향이 정확한 값으로 초기화 되고 계층을 초기화 하는 함수가 추가되었습니다. \n",
    "##### *편향 가중치\n",
    "학습 데이터(Input)이 가중치와 계산되어 넘어야 하는 임계점\n",
    "\n",
    "\n",
    "#### 단일 getter를 갖는 추상 클래스 Layer\n",
    "이 클래스는 더 복잡한 유형의 계층(계층을 포함하는 계층)을 고려하여 작성되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. 계층을 포함하는 계층\n",
    "\n",
    "## 계층은 다른 계층을 포함할 수 있습니다\n",
    "순전파 Layer가 실어 나르는 리스트 안에 담긴 각 Layer의 출력은 그 다음 Layer의 입력이 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드가 간단해진 것을 볼 수 있다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literation :  1\n",
      "pred :  [[ 0.        ]\n",
      " [-0.16322853]\n",
      " [ 0.19133757]\n",
      " [ 0.02810904]]\n",
      "loss :  [2.33428272]\n",
      "====================\n",
      "literation :  2\n",
      "pred :  [[0.21642318]\n",
      " [1.06355293]\n",
      " [0.02706241]\n",
      " [0.87419215]]\n",
      "loss :  [0.06743796]\n",
      "====================\n",
      "literation :  3\n",
      "pred :  [[ 0.20495065]\n",
      " [ 2.04954781]\n",
      " [-0.31382679]\n",
      " [ 1.53077037]]\n",
      "loss :  [1.52375981]\n",
      "====================\n",
      "literation :  4\n",
      "pred :  [[-0.06608193]\n",
      " [ 1.75633489]\n",
      " [-0.37812003]\n",
      " [ 1.4442968 ]]\n",
      "loss :  [0.91678369]\n",
      "====================\n",
      "literation :  5\n",
      "pred :  [[-0.28432789]\n",
      " [ 0.80029251]\n",
      " [ 0.08904433]\n",
      " [ 1.17366473]]\n",
      "loss :  [0.15881376]\n",
      "====================\n",
      "literation :  6\n",
      "pred :  [[-0.34330462]\n",
      " [ 0.18462723]\n",
      " [ 0.48006987]\n",
      " [ 1.00800172]]\n",
      "loss :  [1.01322193]\n",
      "====================\n",
      "literation :  7\n",
      "pred :  [[-0.37194604]\n",
      " [ 0.14538912]\n",
      " [ 0.14934416]\n",
      " [ 0.66667932]]\n",
      "loss :  [1.00210997]\n",
      "====================\n",
      "literation :  8\n",
      "pred :  [[-0.30282278]\n",
      " [ 0.40890071]\n",
      " [-0.30387457]\n",
      " [ 0.40784892]]\n",
      "loss :  [0.88408266]\n",
      "====================\n",
      "literation :  9\n",
      "pred :  [[-0.02489613]\n",
      " [ 0.70410468]\n",
      " [-0.00848608]\n",
      " [ 0.72051473]]\n",
      "loss :  [0.16635789]\n",
      "====================\n",
      "literation :  10\n",
      "pred :  [[0.19901178]\n",
      " [0.9610579 ]\n",
      " [0.37428425]\n",
      " [1.13633037]]\n",
      "loss :  [0.19979685]\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Linear(3,1)]) \n",
    "\n",
    "#weight를 random하게 설정\n",
    "#w = list()\n",
    "#w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "#w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print('literation : ', i+1)\n",
    "    \n",
    "    # 예측 - 간단해진 코드\n",
    "    pred = model.forward(data)   # pred = data.mm(w[0]).mm(w[1])\n",
    "    print('pred : ', pred)\n",
    "    \n",
    "    # loss Function\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    print('loss : ',loss)\n",
    "    \n",
    "    # 학습\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    \n",
    "    print('====================')\n",
    "\n",
    "    \n",
    "data = Tensor(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. 손실 함수 계층\n",
    "\n",
    "## Loss Function까지 Class로 만들어보자\n",
    "평균제곱오차계층으로 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target)*(pred - target)).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literation :  1\n",
      "pred :  [[ 0.        ]\n",
      " [-0.16322853]\n",
      " [ 0.19133757]\n",
      " [ 0.02810904]]\n",
      "loss :  [2.33428272]\n",
      "====================\n",
      "literation :  2\n",
      "pred :  [[0.21642318]\n",
      " [1.06355293]\n",
      " [0.02706241]\n",
      " [0.87419215]]\n",
      "loss :  [0.06743796]\n",
      "====================\n",
      "literation :  3\n",
      "pred :  [[ 0.20495065]\n",
      " [ 2.04954781]\n",
      " [-0.31382679]\n",
      " [ 1.53077037]]\n",
      "loss :  [1.52375981]\n",
      "====================\n",
      "literation :  4\n",
      "pred :  [[-0.06608193]\n",
      " [ 1.75633489]\n",
      " [-0.37812003]\n",
      " [ 1.4442968 ]]\n",
      "loss :  [0.91678369]\n",
      "====================\n",
      "literation :  5\n",
      "pred :  [[-0.28432789]\n",
      " [ 0.80029251]\n",
      " [ 0.08904433]\n",
      " [ 1.17366473]]\n",
      "loss :  [0.15881376]\n",
      "====================\n",
      "literation :  6\n",
      "pred :  [[-0.34330462]\n",
      " [ 0.18462723]\n",
      " [ 0.48006987]\n",
      " [ 1.00800172]]\n",
      "loss :  [1.01322193]\n",
      "====================\n",
      "literation :  7\n",
      "pred :  [[-0.37194604]\n",
      " [ 0.14538912]\n",
      " [ 0.14934416]\n",
      " [ 0.66667932]]\n",
      "loss :  [1.00210997]\n",
      "====================\n",
      "literation :  8\n",
      "pred :  [[-0.30282278]\n",
      " [ 0.40890071]\n",
      " [-0.30387457]\n",
      " [ 0.40784892]]\n",
      "loss :  [0.88408266]\n",
      "====================\n",
      "literation :  9\n",
      "pred :  [[-0.02489613]\n",
      " [ 0.70410468]\n",
      " [-0.00848608]\n",
      " [ 0.72051473]]\n",
      "loss :  [0.16635789]\n",
      "====================\n",
      "literation :  10\n",
      "pred :  [[0.19901178]\n",
      " [0.9610579 ]\n",
      " [0.37428425]\n",
      " [1.13633037]]\n",
      "loss :  [0.19979685]\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "\n",
    "#Loss Function Class\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print('literation : ', i+1)\n",
    "    \n",
    "    # 예측 - 간단해진 코드\n",
    "    pred = model.forward(data)   # pred = data.mm(w[0]).mm(w[1])\n",
    "    print('pred : ', pred)\n",
    "    \n",
    "    # loss Function - 간단해진 코드\n",
    "    loss = criterion.forward(pred, target)   #loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    print('loss : ',loss)\n",
    "    \n",
    "    # 학습\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    \n",
    "    print('====================')\n",
    "\n",
    "    \n",
    "data = Tensor(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *정리 : 프레임워크\n",
    "## 프레임 워크 = 자동미분 + 기본 제공 계층 + 최적화기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. 비선형 계층\n",
    "## Tensor에 'sigmoid'와 'tanh'를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #새롭게 추가된 sigmoid와 tanh의 역전파 논리\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # tanh와 sigmoid의 autograd\n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh 와 Sigmoid Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.06372865]\n",
      "[0.75148144]\n",
      "[0.44681309]\n",
      "[0.123688]\n",
      "[0.0039108]\n",
      "[0.0003632]\n",
      "[0.00066403]\n",
      "[0.00577027]\n",
      "[0.03565456]\n",
      "[0.0092316]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "#model에 tanh와 sigmoid가 들어가있다\n",
    "model = Sequential([Linear(2,3), Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.forward(data)\n",
    "    \n",
    "    # Compare\n",
    "    loss = criterion.forward(pred, target)\n",
    "    \n",
    "    # Learn\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# + relu 함수를 만들어 보자\n",
    "## 역전파 논리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 워드 임베딩(Word Embedding)\n",
    "워드 임베딩(Word Embedding)은 단어를 벡터로 표현하는 것을 말합니다. 워드 임베딩은 단어를 밀집 표현으로 변환하는 방법을 말합니다. 이번 챕터에서는 희소 표현, 밀집 표현, 그리고 워드 임베딩에 대한 개념을 이해합니다\n",
    "\n",
    "## 희소 표현(Sparse Representation)\n",
    "앞서 원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법이었습니다. 이렇게 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 합니다. 그러니까 원-핫 벡터는 희소 벡터(sparse vector)입니다.\n",
    "\n",
    "Ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] # 이 때 1 뒤의 0의 수는 9995개.\n",
    "\n",
    "이러한 벡터 표현은 공간적 낭비를 불러일으킵니다. 잘 생각해보면, 공간적 낭비를 일으키는 것은 원-핫 벡터뿐만은 아닙니다. 희소 표현의 일종인 DTM과 같은 경우에도 특정 문서에 여러 단어가 다수 등장하였으나, 다른 많은 문서에서는 해당 특정 문서에 등장했던 단어들이 전부 등장하지 않는다면 역시나 행렬의 많은 값이 0이 되면서 공간적 낭비를 일으킵니다. 뿐만 아니라, 원-핫 벡터는 단어의 의미를 담지 못한다는 단점을 갖고있습니다.\n",
    "\n",
    "## 밀집 표현(Dense Representation)\n",
    "이러한 희소 표현과 반대되는 표현이 있으니, 이를 밀집 표현(dense representation)이라고 합니다. 밀집 표현은 벡터의 차원을 단어 집합의 크기로 상정하지 않습니다. 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춥니다. 또한, 이 과정에서 더 이상 0과 1만 가진 값이 아니라 실수값을 가지게 됩니다. 다시 희소 표현의 예를 가져와봅시다.\n",
    "\n",
    "Ex) 강아지 = [ 0 0 0 0 1 0 0 0 0 0 0 0 ... 중략 ... 0] # 이 때 1 뒤의 0의 수는 9995개. 차원은 10,000\n",
    "\n",
    "예를 들어 10,000개의 단어가 있을 때 강아지란 단어를 표현하기 위해서는 위와 같은 표현을 사용했습니다. 하지만 밀집 표현을 사용하고, 사용자가 밀집 표현의 차원을 128로 설정한다면, 모든 단어의 벡터 표현의 차원은 128로 바뀌면서 모든 값이 실수가 됩니다.\n",
    "\n",
    "Ex) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...] # 이 벡터의 차원은 128\n",
    "\n",
    "이 경우 벡터의 차원이 조밀해졌다고 하여 밀집 벡터(dense vector)라고 합니다.\n",
    "\n",
    "### 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법을 워드 임베딩(word embedding)이라고 합니다. 그리고 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)라고도 합니다.\n",
    "\n",
    "![title](https://hackernoon.com/drafts/yop32ir.png)\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩 계층\n",
    "## 임베딩 계층은 색인을 활성화로 변환합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # this random initialiation style is just a convention from word2vec\n",
    "        self.weight = (np.random.rand(vocab_size, dim) - 0.5) / dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor에 Autograd를 위한 Indexing 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                \n",
    "                \n",
    "                # 임베딩 계층을 구축할 수 있으려면 Autograd가 indexing을 지원해야 합니다\n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.eye(5), autograd=True)\n",
    "x.index_select(Tensor([[1,2,3],[2,3,4]])).backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서 재사용을 위한 자동 미분 디버깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 1 = a, 2 = b, 3 = c \n",
      "init {}\n",
      "\n",
      "init {}\n",
      "\n",
      "init {}\n",
      "\n",
      "++++++++++++++++\n",
      "d 선언\n",
      "id :  47449\n",
      "init {}\n",
      "47449 is not in children of  1\n",
      " add this id in children of  1 :: c.children :  {47449: 1}\n",
      "47449 is not in children of  2\n",
      " add this id in children of  2 :: c.children :  {47449: 1}\n",
      "\n",
      "e 선언\n",
      "id :  90637\n",
      "init {}\n",
      "90637 is not in children of  2\n",
      " add this id in children of  2 :: c.children :  {47449: 1, 90637: 1}\n",
      "90637 is not in children of  3\n",
      " add this id in children of  3 :: c.children :  {90637: 1}\n",
      "\n",
      "f 선언\n",
      "id :  36444\n",
      "init {}\n",
      "36444 is not in children of  47449\n",
      " add this id in children of  47449 :: c.children :  {36444: 1}\n",
      "36444 is not in children of  90637\n",
      " add this id in children of  90637 :: c.children :  {36444: 1}\n",
      "\n",
      "+++++++++++++==\n",
      "\n",
      "<역전파 시작>\n",
      "id :  53484\n",
      "init {}\n",
      "\n",
      "self.grad of  36444  is None. self.grad = grad :  [1 1 1 1 1]\n",
      "call All_Children! is  True\n",
      "\n",
      "grad_origin is None, minus grad_origin.id ::  {36444: 0}\n",
      "self.grad of  47449  is None. self.grad = grad :  [1 1 1 1 1]\n",
      "call All_Children! is  True\n",
      "\n",
      "grad_origin is None, minus grad_origin.id ::  {47449: 0}\n",
      "self.grad of  1  is None. self.grad = grad :  [1 1 1 1 1]\n",
      "call All_Children! is  True\n",
      "\n",
      "grad_origin is None, minus grad_origin.id ::  {47449: 0, 90637: 1}\n",
      "self.grad of  2  is None. self.grad = grad :  [1 1 1 1 1]\n",
      "call All_Children! is  False\n",
      "\n",
      "++++++역전파 완료++++++\n",
      "\n",
      "grad_origin is None, minus grad_origin.id ::  {36444: 0}\n",
      "self.grad of  90637  is None. self.grad = grad :  [1 1 1 1 1]\n",
      "call All_Children! is  True\n",
      "\n",
      "grad_origin is None, minus grad_origin.id ::  {47449: 0, 90637: 0}\n",
      "id :  75881\n",
      "init {}\n",
      "\n",
      "[2 2 2 2 2]\n",
      "self.grad of  2  is not None [2 2 2 2 2]\n",
      "call All_Children! is  True\n",
      "\n",
      "grad_origin is None, minus grad_origin.id ::  {90637: 0}\n",
      "self.grad of  3  is None. self.grad = grad :  [1 1 1 1 1]\n",
      "call All_Children! is  True\n",
      "\n",
      "++++++역전파 완료++++++\n",
      "\n",
      "++++++역전파 완료++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "            print('id : ', self.id)\n",
    "            \n",
    "        else:\n",
    "            self.id = id\n",
    "    \n",
    "        \n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        print('init',self.children)\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    print(self.id, 'is not in children of ' ,c.id)\n",
    "                    c.children[self.id] = 1\n",
    "                    print(' add this id in children of ',c.id, ':: c.children : ',c.children)\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "                    print(self.id, 'is in ' ,c.id,' add this id in c.children :: c.children : ',c.children)\n",
    "        print()\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "       \n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True        \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad is None):\n",
    "                grad = FloatTensor(np.ones_like(self.data))\n",
    "            \n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    print('grad_origin is not None')\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "                   \n",
    "                    print('grad_origin is None, minus grad_origin.id :: ',self.children)\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "                \n",
    "                print('self.grad of ',self.id,' is None. self.grad = grad : ',self.grad)\n",
    "            else:\n",
    "                self.grad += grad\n",
    "                print(self.grad)\n",
    "                print('self.grad of ',self.id,' is not None',self.grad)\n",
    "            \n",
    "            \n",
    "            # grads must not have grads of their own\n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            # only continue backpropping if there's something to\n",
    "            # backprop into and if all gradients (from children)\n",
    "            # are accounted for override waiting for children if\n",
    "            # \"backprop\" was called on this variable directly\n",
    "            print('call All_Children! is ',self.all_children_grads_accounted_for() )\n",
    "            print()\n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                  \n",
    "                    print('++++++역전파 완료++++++')\n",
    "                    print()\n",
    "                \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "\n",
    "print('id : 1 = a, 2 = b, 3 = c ')\n",
    "a = Tensor([1,2,3,4,5], autograd=True,id=1)\n",
    "b = Tensor([2,2,2,2,2], autograd=True,id=2)\n",
    "c = Tensor([5,4,3,2,1], autograd=True,id=3)\n",
    "print('++++++++++++++++')\n",
    "\n",
    "print('d 선언')\n",
    "d = a + b\n",
    "print('e 선언')\n",
    "e = b + c\n",
    "print('f 선언')\n",
    "f = d + e\n",
    "\n",
    "print('+++++++++++++==')\n",
    "\n",
    "print()\n",
    "print('<역전파 시작>')\n",
    "\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
