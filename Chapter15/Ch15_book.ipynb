{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch15.book.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Ua4rQjwRuA",
        "colab_type": "text"
      },
      "source": [
        "# **Ch 15.  보이지 않는 데이터로 하는 딥러닝  :  통합 학습 입문**\n",
        "------------------------\n",
        "## **딥러닝의 개인정보 문제**\n",
        "- 딥러닝은 여러분이 학습 데이터에 접근할 수 있다는 것을 의미합니다.\n",
        "\n",
        "> 머신러닝의 하위 분야인 딥러닝은 데이터 학습이 전부라고 할 수 있는 만큼 개인적인 정보를 다루는 일이 빈번하다. 딥러닝 모델은 자신을 이해할 수 있도록 하기 위해 타인 수천 명의 개인 정보를 학습할 수 있다는 말이다.\n",
        ">\n",
        "> 학습 데이터 없이는 딥러닝이 학습할 수 있는 것은 아무 것도 없다. 딥러닝의 가치를 높에 해주는 용도가 가장 개인적인 데이터셋과 교류하는 것이다 보니, 기업들은 종종 딥러닝 때문에 테이터를 수집하려 한다. 기업이 특정 문제를 해결할려면 개인적인 정보가 불가피하게 필요하기 때문이다.\n",
        ">\n",
        "> 그래서 구글에서 모델 학습 때문에 굳이 데이터 셋을 모으지 않아도 되는 기법을 제안했다. \"모든 데이터를 한 장소에 모으는 대신, 모델을 데이터 쪽으로 가져가면 어떨까?\" 라는 부분에서 탄생된 것이 바로 이번 장에 다룰 **통합 학습(federated learning)**이다.\n",
        ">\n",
        "> 이 통합 학습을 이용하면 딥러닝 공급망 사슬에 참여하기 위해 자신의 개인정보를 보낼 필요가 없어진다. 헬스케어, 인사 관리, 기타 다른 민감한 영역의 귀중한 모델들도 사람들에게 개인정보를 공개해달라고 하지 않아도 학습이 가능해진다는 것을 의미한다. 이 기법을 이용해 고객 정보를 공유할 수 없었던 대기업들이 해당 데이터들로 수익을 확보할 수 있게 되었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL-AnbBUzFyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "    \n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,1000000000)\n",
        "        else:\n",
        "            self.id = id\n",
        "        \n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "        \n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True \n",
        "        \n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        " \n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    return\n",
        "                    print(self.id)\n",
        "                    print(self.creation_op)\n",
        "                    print(len(self.creators))\n",
        "                    for c in self.creators:\n",
        "                        print(c.creation_op)\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "            \n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "            \n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if(self.creators is not None and \n",
        "               (self.all_children_grads_accounted_for() or \n",
        "                grad_origin is None)):\n",
        "\n",
        "                if(self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "                    \n",
        "                if(self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if(self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)                    \n",
        "                    \n",
        "                if(self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "                    \n",
        "                if(self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if(\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,\n",
        "                                                               self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if(\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "                    \n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "                    \n",
        "                if(self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "                \n",
        "                if(self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
        "                \n",
        "                if(self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "                    \n",
        "                if(self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "                    \n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)    \n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "    \n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "        \n",
        "        if(self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "    \n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "        \n",
        "        return Tensor(self.data.transpose())\n",
        "    \n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "    \n",
        "    def sigmoid(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(np.tanh(self.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "    \n",
        "    def index_select(self, indices):\n",
        "\n",
        "        if(self.autograd):\n",
        "            new = Tensor(self.data[indices.data],\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "    \n",
        "    def softmax(self):\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        return softmax_output\n",
        "    \n",
        "    def cross_entropy(self, target_indices):\n",
        "\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        \n",
        "        t = target_indices.data.flatten()\n",
        "        p = softmax_output.reshape(len(t),-1)\n",
        "        target_dist = np.eye(p.shape[1])[t]\n",
        "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "    \n",
        "        if(self.autograd):\n",
        "            out = Tensor(loss,\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "        \n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())  \n",
        "\n",
        "class Layer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "    \n",
        "class SGD(object):\n",
        "    \n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "        \n",
        "    def step(self, zero=True):\n",
        "        \n",
        "        for p in self.parameters:\n",
        "            \n",
        "            p.data -= p.grad.data * self.alpha\n",
        "            \n",
        "            if(zero):\n",
        "                p.grad.data *= 0\n",
        "\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.use_bias = bias\n",
        "        \n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        if(self.use_bias):\n",
        "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "        \n",
        "        if(self.use_bias):        \n",
        "            self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if(self.use_bias):\n",
        "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
        "        return input.mm(self.weight)\n",
        "\n",
        "\n",
        "class Sequential(Layer):\n",
        "    \n",
        "    def __init__(self, layers=list()):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = layers\n",
        "    \n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params\n",
        "\n",
        "\n",
        "class Embedding(Layer):\n",
        "    \n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        \n",
        "        # this random initialiation style is just a convention from word2vec\n",
        "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.weight.index_select(input)\n",
        "\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()\n",
        "    \n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        return input.cross_entropy(target)\n",
        "\n",
        "class MSELoss(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        dif = input - target\n",
        "        return (dif * dif).sum(0)\n",
        "    \n",
        "class RNNCell(Layer):\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "        \n",
        "        if(activation == 'sigmoid'):\n",
        "            self.activation = Sigmoid()\n",
        "        elif(activation == 'tanh'):\n",
        "            self.activation == Tanh()\n",
        "        else:\n",
        "            raise Exception(\"Non-linearity not found\")\n",
        "\n",
        "        self.w_ih = Linear(n_inputs, n_hidden)\n",
        "        self.w_hh = Linear(n_hidden, n_hidden)\n",
        "        self.w_ho = Linear(n_hidden, n_output)\n",
        "        \n",
        "        self.parameters += self.w_ih.get_parameters()\n",
        "        self.parameters += self.w_hh.get_parameters()\n",
        "        self.parameters += self.w_ho.get_parameters()        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        from_prev_hidden = self.w_hh.forward(hidden)\n",
        "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
        "        new_hidden = self.activation.forward(combined)\n",
        "        output = self.w_ho.forward(new_hidden)\n",
        "        return output, new_hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "    \n",
        "class LSTMCell(Layer):\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_output):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "\n",
        "        self.xf = Linear(n_inputs, n_hidden)\n",
        "        self.xi = Linear(n_inputs, n_hidden)\n",
        "        self.xo = Linear(n_inputs, n_hidden)        \n",
        "        self.xc = Linear(n_inputs, n_hidden)        \n",
        "        \n",
        "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hc = Linear(n_hidden, n_hidden, bias=False)        \n",
        "        \n",
        "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
        "        \n",
        "        self.parameters += self.xf.get_parameters()\n",
        "        self.parameters += self.xi.get_parameters()\n",
        "        self.parameters += self.xo.get_parameters()\n",
        "        self.parameters += self.xc.get_parameters()\n",
        "\n",
        "        self.parameters += self.hf.get_parameters()\n",
        "        self.parameters += self.hi.get_parameters()        \n",
        "        self.parameters += self.ho.get_parameters()        \n",
        "        self.parameters += self.hc.get_parameters()                \n",
        "        \n",
        "        self.parameters += self.w_ho.get_parameters()        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        \n",
        "        prev_hidden = hidden[0]        \n",
        "        prev_cell = hidden[1]\n",
        "        \n",
        "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
        "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
        "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        \n",
        "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        \n",
        "        c = (f * prev_cell) + (i * g)\n",
        "\n",
        "        h = o * c.tanh()\n",
        "        \n",
        "        output = self.w_ho.forward(h)\n",
        "        return output, (h, c)\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "        init_hidden.data[:,0] += 1\n",
        "        init_cell.data[:,0] += 1\n",
        "        return (init_hidden, init_cell)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgiNFQr6zI2V",
        "colab_type": "text"
      },
      "source": [
        "## **통합 학습**\n",
        "\n",
        "- 학습이 목적이라면 데이터셋에 대한 접근 권한이 필요하진 않습니다.\n",
        "\n",
        "> 통합 학습은 보안 환경으로 들어가서 데이터를 옮기지 않고도 문제를 해결하는 방법을 학습하는 모델에 관한 것이다. 코드를 살펴보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYYBPkXkwNM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "import sys\n",
        "np.random.seed(12345)\n",
        "\n",
        "import codecs\n",
        "with codecs.open('spam.txt', \"r\",encoding='utf-8', errors='ignore') as fdata:\n",
        "    raw = fdata.readlines()\n",
        "\n",
        "vocab = set()\n",
        "    \n",
        "spam = list()\n",
        "for row in raw:\n",
        "    spam.append(set(row[:-2].split(\" \")))\n",
        "    for word in spam[-1]:\n",
        "        vocab.add(word)\n",
        "    \n",
        "import codecs\n",
        "with codecs.open('ham.txt', \"r\",encoding='utf-8', errors='ignore') as fdata:\n",
        "    raw = fdata.readlines()\n",
        "\n",
        "ham = list()\n",
        "for row in raw:\n",
        "    ham.append(set(row[:-2].split(\" \")))\n",
        "    for word in ham[-1]:\n",
        "        vocab.add(word)\n",
        "        \n",
        "vocab.add(\"<unk>\")\n",
        "\n",
        "vocab = list(vocab)\n",
        "w2i = {}\n",
        "for i,w in enumerate(vocab):\n",
        "    w2i[w] = i\n",
        "    \n",
        "def to_indices(input, l=500):\n",
        "    indices = list()\n",
        "    for line in input:\n",
        "        if(len(line) < l):\n",
        "            line = list(line) + [\"<unk>\"] * (l - len(line))\n",
        "            idxs = list()\n",
        "            for word in line:\n",
        "                idxs.append(w2i[word])\n",
        "            indices.append(idxs)\n",
        "    return indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_vxzt511ZfT",
        "colab_type": "text"
      },
      "source": [
        "## **스팸 탐지 학습**\n",
        "- 실제 사용자의 이메일을 이용해서 스팸 탐지 모델을 학습하고 싶다고 합시다.\n",
        "\n",
        "> 첫 번째 모델은 엔론 데이터셋(유명한 엔론 소송을 통해 알려진 대형 이메일 말뭉치)이라고 하는 공개 데이터셋을 이용해서 학습한다. 이 데이터셋에는 여러 가지 개인적인 정보가 담겨있는데 법정 소송을 통해 대중에게 모두 공개되었다고 한다.\n",
        ">\n",
        "> 앞 코드와 이번 코드는 전처리(preprocessin)만 수행한다. 13장에서 딥러닝 프레임워크를 만들 때 생성했던 임베딩 클래스로 순전파를 넣을 수 있도록 전처리 과정이 필수적이다. 앞서 수행한 코드와 같이 이 말뭉치 속의 모든 단어는 인덱스된 목록으로 변환하는 과정을 거친다. 그리고 모든 이메일을 추리거나 `<unk>` 토큰으로 채워 넣어서 정확히 500개의 단어로만 구성되도록 바꿔야한다. 그래서 최종 데이터 셋은 결과적으로 **정방형**이 된다.\n",
        ">\n",
        "> 참고 ) unk는 unknown token의 줄임말이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap-Q7nq906VV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spam_idx = to_indices(spam)\n",
        "ham_idx = to_indices(ham)\n",
        "\n",
        "train_spam_idx = spam_idx[0:-1000]\n",
        "train_ham_idx = ham_idx[0:-1000]\n",
        "\n",
        "test_spam_idx = spam_idx[-1000:]\n",
        "test_ham_idx = ham_idx[-1000:]\n",
        "\n",
        "train_data = list()\n",
        "train_target = list()\n",
        "\n",
        "test_data = list()\n",
        "test_target = list()\n",
        "\n",
        "for i in range(max(len(train_spam_idx),len(train_ham_idx))):\n",
        "    train_data.append(train_spam_idx[i%len(train_spam_idx)])\n",
        "    train_target.append([1])\n",
        "    \n",
        "    train_data.append(train_ham_idx[i%len(train_ham_idx)])\n",
        "    train_target.append([0])\n",
        "    \n",
        "for i in range(max(len(test_spam_idx),len(test_ham_idx))):\n",
        "    test_data.append(test_spam_idx[i%len(test_spam_idx)])\n",
        "    test_target.append([1])\n",
        "    \n",
        "    test_data.append(test_ham_idx[i%len(test_ham_idx)])\n",
        "    test_target.append([0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCsLJqU91YuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, input_data, target_data, batch_size=500, iterations=5):\n",
        "    \n",
        "    criterion = MSELoss()\n",
        "    optim = SGD(parameters=model.get_parameters(), alpha=0.01)\n",
        "    \n",
        "    n_batches = int(len(input_data) / batch_size)\n",
        "    bs = batch_size\n",
        "    for iter in range(iterations):\n",
        "        iter_loss = 0\n",
        "        for b_i in range(n_batches):\n",
        "\n",
        "            # padding token should stay at 0\n",
        "            model.weight.data[w2i['<unk>']] *= 0 \n",
        "            input = Tensor(input_data[b_i*bs:(b_i+1)*bs], autograd=True)\n",
        "            target = Tensor(target_data[b_i*bs:(b_i+1)*bs], autograd=True)\n",
        "\n",
        "            pred = model.forward(input).sum(1).sigmoid()\n",
        "            loss = criterion.forward(pred,target)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            iter_loss += loss.data[0] / bs\n",
        "\n",
        "            sys.stdout.write(\"\\r\\tLoss:\" + str(iter_loss / (b_i+1)))\n",
        "        print()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPhhvNpP2y9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_input, test_output):\n",
        "    \n",
        "    model.weight.data[w2i['<unk>']] *= 0 \n",
        "    \n",
        "    input = Tensor(test_input, autograd=True)\n",
        "    target = Tensor(test_output, autograd=True)\n",
        "\n",
        "    pred = model.forward(input).sum(1).sigmoid()\n",
        "    return ((pred.data > 0.5) == target.data).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBwNKFlL3SZH",
        "colab_type": "text"
      },
      "source": [
        "위와 같이  train()과 test() 함수를 이용하면 이어지는 코드를 이용해 신경망을 초기화하고 학습시킬 수 있다. 테스트 데이터셋의 균형이 잡혀있는 상태라 세 번만 돌려도 99% 정도의 정확도로 분류를 수행할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsdxS6d13N6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Embedding(vocab_size=len(vocab), dim=1)\n",
        "model.weight.data *= 0\n",
        "criterion = MSELoss()\n",
        "optim = SGD(parameters=model.get_parameters(), alpha=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLfZBEw27RXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "5a651713-ac54-4605-9916-f7124e847c2a"
      },
      "source": [
        "for i in range(3):\n",
        "    model = train(model, train_data, train_target, iterations=1)\n",
        "    print(\"% Correct on Test Set: \" + str(test(model, test_data, test_target)*100))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tLoss:0.04895785149340228\n",
            "% Correct on Test Set: 98.75\n",
            "\tLoss:0.014751484575885144\n",
            "% Correct on Test Set: 99.15\n",
            "\tLoss:0.01073762530961579\n",
            "% Correct on Test Set: 99.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKjSrhsR7n1G",
        "colab_type": "text"
      },
      "source": [
        "## **통합해봅시다**\n",
        "\n",
        "- 앞 예제 코드는 평범한 바닐라 딥러닝이었습니다. 이제 개인정보를 보호해볼까요?\n",
        "\n",
        "앞선 예제는 모든 이메일을 한 곳에 모아서 딥러닝을 돌렸다. 이번에는 여러 개의 다양한 이메일 컬렉션을 가지는 **통합 학습 환경**을 시뮬레이션해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTUsc5x37lT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bob = (train_data[0:1000], train_target[0:1000])\n",
        "alice = (train_data[1000:2000], train_target[1000:2000])\n",
        "sue = (train_data[2000:], train_target[2000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGWBvrmP8MQf",
        "colab_type": "text"
      },
      "source": [
        "위 코드는 이전과 동일한 학습을 할 수 있지만, 이번에는 모든 사람의 이메일 데이터베이스를 동시에 처리한다. 각 반복을 수행한 후에, Bob, Alice, Sue로부터 나온 모델 값에 대해 평균을 내고 평가한다. 참고로 통합 학습의 어떤 파생 기법들은 각 배치 후에 합치기도 한다. 여기서는 간단히 처리하였다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad__62HN827M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Embedding(vocab_size=len(vocab), dim=1)\n",
        "model.weight.data *= 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_IvRt8K8Kwj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "fa4fd564-f9c0-41db-be3f-263ef44723e9"
      },
      "source": [
        "import copy\n",
        "\n",
        "for i in range(3):\n",
        "    print(\"Starting Training Round...\")\n",
        "    print(\"\\tStep 1: send the model to Bob\")\n",
        "    bob_model = train(copy.deepcopy(model), bob[0], bob[1], iterations=1)\n",
        "    \n",
        "    print(\"\\n\\tStep 2: send the model to Alice\")\n",
        "    alice_model = train(copy.deepcopy(model), alice[0], alice[1], iterations=1)\n",
        "    \n",
        "    print(\"\\n\\tStep 3: Send the model to Sue\")\n",
        "    sue_model = train(copy.deepcopy(model), sue[0], sue[1], iterations=1)\n",
        "    \n",
        "    print(\"\\n\\tAverage Everyone's New Models\")\n",
        "    model.weight.data = (bob_model.weight.data + \\\n",
        "                         alice_model.weight.data + \\\n",
        "                         sue_model.weight.data)/3\n",
        "    \n",
        "    print(\"\\t% Correct on Test Set: \" + \\\n",
        "          str(test(model, test_data, test_target)*100))\n",
        "    \n",
        "    print(\"\\nRepeat!!\\n\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training Round...\n",
            "\tStep 1: send the model to Bob\n",
            "\tLoss:0.21908166249699718\n",
            "\n",
            "\tStep 2: send the model to Alice\n",
            "\tLoss:0.2937106899184867\n",
            "\n",
            "\tStep 3: Send the model to Sue\n",
            "\tLoss:0.04370172144365995\n",
            "\n",
            "\tAverage Everyone's New Models\n",
            "\t% Correct on Test Set: 87.0\n",
            "\n",
            "Repeat!!\n",
            "\n",
            "Starting Training Round...\n",
            "\tStep 1: send the model to Bob\n",
            "\tLoss:0.07604929485450417\n",
            "\n",
            "\tStep 2: send the model to Alice\n",
            "\tLoss:0.10877523800360037\n",
            "\n",
            "\tStep 3: Send the model to Sue\n",
            "\tLoss:0.025948541543226986\n",
            "\n",
            "\tAverage Everyone's New Models\n",
            "\t% Correct on Test Set: 92.7\n",
            "\n",
            "Repeat!!\n",
            "\n",
            "Starting Training Round...\n",
            "\tStep 1: send the model to Bob\n",
            "\tLoss:0.03756810302218815\n",
            "\n",
            "\tStep 2: send the model to Alice\n",
            "\tLoss:0.04502773446215378\n",
            "\n",
            "\tStep 3: Send the model to Sue\n",
            "\tLoss:0.019466797770650156\n",
            "\n",
            "\tAverage Everyone's New Models\n",
            "\t% Correct on Test Set: 98.75\n",
            "\n",
            "Repeat!!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rWtr6Bl9H3k",
        "colab_type": "text"
      },
      "source": [
        "위 코드에서는 예전 모델과 거의 동일한  성능을 보여주고 있지만 접근 권한이 있는지에 대해서는 모호성을 띠고 있다. 이 사람들의 데이터셋에 대한 정보를 전혀 알아낼 수 없을까? \n",
        "\n",
        "## **통합 학습 해킹하기**\n",
        "\n",
        "- 장난감 예제를 이용해 데이터셋을 학습하는 방법을 관찰해봅시다.\n",
        "\n",
        "> 통합 학습에는 두 가지 문제점이 있다. 이 문제 모두 학습 데이터셋 안에 각 갠인이 소량의 데이터 예제만 가지고 있을 경우에만 발생한다. 이는 성능과 개인정보 보호에 관련된 것인데, 누군가가 소수의 학습 예제만 갖고 있는 경우에는 데이터에 대해 많은 것을 파악할 수 있는 것으로 드러났다. 게다가 예를 들어 10,000명의 데이터(각각이 소수 정보를 제공)가 제공되면, (특히 모델을 교환하는 데 대부분의 시간을 사용하고 학습에는 시간을 별로 할애하지 못하는 문제점을 갖고 있었다.\n",
        ">\n",
        "> 사용자가 단일 배치상에서 가중치 갱신을 수행할 때 무엇을 학습할 수 있는지 살펴보자. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z91eg45a80yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc843659-5f4b-430b-f7a2-753878027adb"
      },
      "source": [
        "import copy\n",
        "\n",
        "bobs_email = [\"my\", \"computer\", \"password\", \"is\", \"pizza\"]\n",
        "\n",
        "bob_input = np.array([[w2i[x] for x in bobs_email]])\n",
        "bob_target = np.array([[0]])\n",
        "\n",
        "model = Embedding(vocab_size=len(vocab), dim=1)\n",
        "model.weight.data *= 0\n",
        "\n",
        "bobs_model = train(copy.deepcopy(model), bob_input, bob_target, iterations=1, batch_size=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\tLoss:0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzDVl625-gyE",
        "colab_type": "text"
      },
      "source": [
        ">Bob은 받은 편지함 안에 있는 이메일을 이용해 모델을 생성하고 갱신하려 한다. 그런데 Bob은 암호를 이메일 안에 저장해뒀다.\n",
        ">\n",
        ">이메일 내용은 \"My computer passward is pizza\"이다. 이제 어떤 가중치가 바뀌었는지 살펴보면 Bob이 받은 이메일의 어휘를 파악하거나 추론할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnvaFKq8-YTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d961347c-3b3f-4810-ea0d-47d5e6951209"
      },
      "source": [
        "for i, v in enumerate(bobs_model.weight.data - model.weight.data):\n",
        "    if(v != 0):\n",
        "        print(vocab[i])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "computer\n",
            "pizza\n",
            "password\n",
            "my\n",
            "is\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QezYOcmb_K-S",
        "colab_type": "text"
      },
      "source": [
        ">위와 같이 이메일 내용을 알아냈다. 학습 데이터셋이 가중치 갱신 과정에서 얻은 정보가 무엇인지 쉽게 알아낼 수 있다면 통합 학습을 제대로 사용할 수 있을까?\n",
        ">\n",
        "\n",
        "\n",
        "## **보안 통합**\n",
        "-누가 보기 전에 방대한 개인 데이터에 대한 가중치 갱신값을 평균화하세요.\n",
        " > 이 문제의 해결책은 Bob이 절대로 경사도를 공개된 곳에 두지 않도록 하는 것이다. 하지만 사람들이 보지 못하도록 해놓은 자신만의 경사도에 Bob이 기여할 수 있는 방법은 무엇일까? 이는 사회 과학에서 사용하는 **무작위 응답(randomized response)** 기법을 잠깐 살펴보자.\n",
        ">\n",
        "> 예시로 100명에게 범죄를 저질렀는지를 물어보는 설문을 진행하는 것을 들고 있다. 이는 모든 대답이 \"아니오\"로 나올 것이다. 설문 결과를 발설하지 않겠다고 약속을 해도 결과는 같을 것이다. 하지만 동전을 두 번 던져서 처음 던졌을 때 숫자가 나오는 경우에만 피설문자가 정직하게 응답해야 한다고 해보자. 그림이 나왔을 때는 두 번째 던진 결과에 따라 \"예\" 또는 \"아니오\"라고 답하면 된다.\n",
        ">\n",
        "> 위 실험을 진행하면 진짜 응답은 첫 번째 동전 던지기와 두 번째 동전던지기의 무작위적인 잡음 뒤에 숨게 된다. 만약 응답자의 60%가 \"예\"라고 답한다면, 약 70%의 응답자가 범죄를 저질렀다고 판단할 수 있다. 그러니까 어떤 사람에 관한 정보가 그 사람이 아닌 잡음으로부터 나올 수 있음을 무작위적 잡음이 합리화시킨다는 점이 이 발상의 요지다.\n",
        ">\n",
        "> - \"그럴듯한 부인\"을 통한 개인정보 보호 / 개인이 아닌 무작위적 잡음으로부터 특정 답변이 나올 가능성이 '**그럴듯한 부인(plausible deniability)**'이라는 명분을 제공함으로써 사람들의 개인정보를 보호해준다. 이는 보안 통합과 더불어 차별적인 개인정보 보호를 위한 기반을 형성한다.\n",
        ">\n",
        "> 자신의 경사도를 제외한 다른 사람의 볼 수 없도록 하는 방식으로 모든 참가자의 경사도를 종합하는 것이 좋다. 이런 문제를 다루는 과업을 일컬어 **보안 통합(secureaggregation)**이라고 하며, **동형 암호화(homomorphic encryption)**라는 도구를 사용해야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4XLRUBqCGhT",
        "colab_type": "text"
      },
      "source": [
        "## **동형 암호화**\n",
        "- 암호화된 값도 수리 연산이 가능합니다.\n",
        "> 가장 흥미로운 미개척 연구 분야는 딥러닝을 포함한 인공지능과 암호학, 이 두 분야의 교집합 분야다. 이 교집합 분야의 전면과 중심에는 동형 암호화라고 하는 것이 있는데 이는 쉽게 말해 암호화된 값을 복호화하지 않은 상태에서 계산이 가능하도록 하는 기술이다.\n",
        ">\n",
        "> 특히, 암호화된 값 사이의 덧셈이 필요한데 이 책에서는 동형 암호화의 몇 가지 정의와 함께 어떻게 동작하는 지 정도만 보여주고 있다.\n",
        ">\n",
        "> 미리 숙지해야 할 개념!\n",
        "> \n",
        "> 공개 키(public key) : 숫자를 암호화할 때 사용한다.\n",
        "> 개인 키(private key) : 암호화된 숫자를 복호화할 때 사용한다.\n",
        "> 암호문(ciphertext) : 암호화된 값\n",
        "> 평문(plaintext) : 암호화되지 않은 값\n",
        "\n",
        "> 아래 코드는 phe 라이브러리를 이용한 동형 암호와 예제이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmLia-bULD8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "294cde8e-e009-4c83-e6b9-d40fdb2378bc"
      },
      "source": [
        "pip install phe"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting phe\n",
            "  Downloading https://files.pythonhosted.org/packages/32/0e/568e97b014eb14e794a1258a341361e9da351dc6240c63b89e1541e3341c/phe-1.4.0.tar.gz\n",
            "Building wheels for collected packages: phe\n",
            "  Building wheel for phe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for phe: filename=phe-1.4.0-py2.py3-none-any.whl size=37362 sha256=49c492981a5d788552d86aac4a83407ba10965db7559da610532ecda3cefddd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/dc/36/dcb6bf0f1b9907e7b710ace63e64d08e7022340909315fdea4\n",
            "Successfully built phe\n",
            "Installing collected packages: phe\n",
            "Successfully installed phe-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe110Gnc-91K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2c459dc-382e-47ad-fe69-ca2cae9b5233"
      },
      "source": [
        "import phe\n",
        "\n",
        "public_key, private_key = phe.generate_paillier_keypair(n_length=1024)\n",
        "\n",
        "# encrypt the number \"5\"\n",
        "x = public_key.encrypt(5)\n",
        "\n",
        "# encrypt the number \"3\"\n",
        "y = public_key.encrypt(3)\n",
        "\n",
        "# add the two encrypted values\n",
        "z = x + y\n",
        "\n",
        "# decrypt the result\n",
        "z_ = private_key.decrypt(z)\n",
        "print(\"The Answer: \" + str(z_))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Answer: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC0tO8HiL57P",
        "colab_type": "text"
      },
      "source": [
        ">위 코드는 두 수, 5와 3을 암호화 한 뒤, 암호화 상태에서 두 수를 더한다. 동형 암호화와 유사한 기능을 구현하는 것이 있는데 바로 **보안 다중 계산(secure mulitipart computation)**이다. 이는 다루지 않는다.\n",
        ">\n",
        "> 다시 보안 통합 문제로 돌아가면 볼 수 없는 수를 더할 수 있다는 점을 고려하게 되면 간단하게 수행할 수 있다. 모델을 초기화하는 사람은 공개 키를 Bob, Alice, Sue에게 보내 그들의 가중치를 암호화하도록 한다. 그다음, 개인 키를 가지고 있지 않은 Bob, Alice, Sue는 서로 교류하며 자신들의 경사도를 하나의 최종 갱신값으로 축적한 뒤에 개인 키를 이용해 복호화할 수 있는 권한을 가진 모델 소유자에게 보낸다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI0pF7dmMotV",
        "colab_type": "text"
      },
      "source": [
        "## **동형 암호화 통합 학습**\n",
        "- 통합되는 경사도 보호를 위해 동형 암호화를 이용해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFlJXRofE8IO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Embedding(vocab_size=len(vocab), dim=1)\n",
        "model.weight.data *= 0\n",
        "\n",
        "# note that in production the n_length should be at least 1024\n",
        "public_key, private_key = phe.generate_paillier_keypair(n_length=128)\n",
        "\n",
        "def train_and_encrypt(model, input, target, pubkey):\n",
        "    new_model = train(copy.deepcopy(model), input, target, iterations=1)\n",
        "\n",
        "    encrypted_weights = list()\n",
        "    for val in new_model.weight.data[:,0]:\n",
        "        encrypted_weights.append(public_key.encrypt(val))\n",
        "    ew = np.array(encrypted_weights).reshape(new_model.weight.data.shape)\n",
        "    \n",
        "    return ew"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB_awBORMw2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "fed0a128-0a56-4ac4-fd09-cb48fd786c0c"
      },
      "source": [
        "for i in range(3):\n",
        "    print(\"\\nStarting Training Round...\")\n",
        "    print(\"\\tStep 1: send the model to Bob\")\n",
        "    bob_encrypted_model = train_and_encrypt(copy.deepcopy(model), \n",
        "                                            bob[0], bob[1], public_key)\n",
        "\n",
        "    print(\"\\n\\tStep 2: send the model to Alice\")\n",
        "    alice_encrypted_model = train_and_encrypt(copy.deepcopy(model), \n",
        "                                              alice[0], alice[1], public_key)\n",
        "\n",
        "    print(\"\\n\\tStep 3: Send the model to Sue\")\n",
        "    sue_encrypted_model = train_and_encrypt(copy.deepcopy(model), \n",
        "                                            sue[0], sue[1], public_key)\n",
        "\n",
        "    print(\"\\n\\tStep 4: Bob, Alice, and Sue send their\")\n",
        "    print(\"\\tencrypted models to each other.\")\n",
        "    aggregated_model = bob_encrypted_model + \\\n",
        "                       alice_encrypted_model + \\\n",
        "                       sue_encrypted_model\n",
        "\n",
        "    print(\"\\n\\tStep 5: only the aggregated model\")\n",
        "    print(\"\\tis sent back to the model owner who\")\n",
        "    print(\"\\t can decrypt it.\")\n",
        "    raw_values = list()\n",
        "    for val in sue_encrypted_model.flatten():\n",
        "        raw_values.append(private_key.decrypt(val))\n",
        "    model.weight.data = np.array(raw_values).reshape(model.weight.data.shape)/3\n",
        "\n",
        "    print(\"\\t% Correct on Test Set: \" + \\\n",
        "              str(test(model, test_data, test_target)*100))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting Training Round...\n",
            "\tStep 1: send the model to Bob\n",
            "\tLoss:0.21908166249699718\n",
            "\n",
            "\tStep 2: send the model to Alice\n",
            "\tLoss:0.2937106899184867\n",
            "\n",
            "\tStep 3: Send the model to Sue\n",
            "\tLoss:0.04370172144365995\n",
            "\n",
            "\tStep 4: Bob, Alice, and Sue send their\n",
            "\tencrypted models to each other.\n",
            "\n",
            "\tStep 5: only the aggregated model\n",
            "\tis sent back to the model owner who\n",
            "\t can decrypt it.\n",
            "\t% Correct on Test Set: 98.75\n",
            "\n",
            "Starting Training Round...\n",
            "\tStep 1: send the model to Bob\n",
            "\tLoss:0.06612036480821967\n",
            "\n",
            "\tStep 2: send the model to Alice\n",
            "\tLoss:0.0680059577116942\n",
            "\n",
            "\tStep 3: Send the model to Sue\n",
            "\tLoss:0.03436716023928057\n",
            "\n",
            "\tStep 4: Bob, Alice, and Sue send their\n",
            "\tencrypted models to each other.\n",
            "\n",
            "\tStep 5: only the aggregated model\n",
            "\tis sent back to the model owner who\n",
            "\t can decrypt it.\n",
            "\t% Correct on Test Set: 98.9\n",
            "\n",
            "Starting Training Round...\n",
            "\tStep 1: send the model to Bob\n",
            "\tLoss:0.06227552561089021\n",
            "\n",
            "\tStep 2: send the model to Alice\n",
            "\tLoss:0.06738138277451584\n",
            "\n",
            "\tStep 3: Send the model to Sue\n",
            "\tLoss:0.03374911228415595\n",
            "\n",
            "\tStep 4: Bob, Alice, and Sue send their\n",
            "\tencrypted models to each other.\n",
            "\n",
            "\tStep 5: only the aggregated model\n",
            "\tis sent back to the model owner who\n",
            "\t can decrypt it.\n",
            "\t% Correct on Test Set: 98.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XYgqH98M521",
        "colab_type": "text"
      },
      "source": [
        "> 이제 새로운 학습 계획을 실행할 수 있게 되었다. 이 학습 계획은 예전에 비해 한 가지 단계가 더 추가되었다. Alice, Bob, Sue는 자신들의 동형 암호화 모델을 반환하기에 앞서 이들을 더한다, 그 때문에 **누구도 어떤 사람의 어떤 경사도가 갱신되었는지 알 수 없다.**('그럴 듯한 부인'의 한 형태이다.) 실전에서는 어떤 무작위 noise를 추가하는 것만으로도 Bob. Alice, Sue의 개인정보를 어느 정도 보호할 수 있는 수준을 충족할 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFg13vxENmAP",
        "colab_type": "text"
      },
      "source": [
        "## **요약**\n",
        "\n",
        "- 통합 학습은 딥러닝에서 가장 흥미진진한 약진 중 하나이다.\n",
        "> 통합 학습을 이용하므로써 예전에는 손댈 수 없을 정도로 민감했던 데이터셋을 새롭게 사용할 수 있을 것이다. 인공지능과 암호화 연구 사이의 교집합, 즉 폭넓은 융합인 통합 학습이 최근 10년 동안 나타났던 가장 흥미진진한 융합 영역이다.\n",
        ">\n",
        "> 통합 학습을 실전에서 사용할 수 없게 하는 주요 장애물은 현대 딥러닝 toolkit에서 이 기술을 지원하지 않는다는 것이다. pip install.. 을 실행해서 통합 학습을 사용할 수 있을 때가 tipping point가 될 것이다. 그때는 개인정보와 보안이 이들 시민이며, 통합 학습, 동형 암호화, 차등적 개인정보 보호, 보안 다중 계산 기능이 내장된 딥러닝 framework를 사용할 수 있을 것이다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmYSw99lMzyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}